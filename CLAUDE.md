# BO Inteligente

Sistema de IA para auxiliar policiais militares na reda√ß√£o de Boletins de Ocorr√™ncia de tr√°fico de drogas.

## Links de Produ√ß√£o

| Ambiente | URL |
|----------|-----|
| Frontend | https://criscmaia.github.io/bo-assistant/ |
| Backend API | https://bo-assistant-backend.onrender.com |
| Dashboard Logs | https://criscmaia.github.io/bo-assistant/logs.html |
| Reposit√≥rio | https://github.com/criscmaia/bo-assistant |

## Stack T√©cnica

- **Backend**: Python 3.13 + FastAPI + SQLAlchemy
- **Frontend**: HTML5 + Vanilla JavaScript + Tailwind CSS (via CDN)
- **LLM**: Google Gemini 2.5 Flash (20 req/dia) + Groq Llama 3.3 70B (14.4k req/dia)
- **Banco de Dados**: PostgreSQL (produ√ß√£o) / SQLite (local)
- **Deploy**: Render (backend) + GitHub Pages (frontend)

## Estrutura do Projeto

```
bo-assistant/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                    # API FastAPI (endpoints)
‚îÇ   ‚îú‚îÄ‚îÄ state_machine.py           # Fluxo Se√ß√£o 1 (6 perguntas)
‚îÇ   ‚îú‚îÄ‚îÄ state_machine_section2.py  # Fluxo Se√ß√£o 2 (8 perguntas)
‚îÇ   ‚îú‚îÄ‚îÄ llm_service.py             # Integra√ß√£o Gemini + Groq
‚îÇ   ‚îú‚îÄ‚îÄ validator.py               # Valida√ß√£o Se√ß√£o 1
‚îÇ   ‚îú‚îÄ‚îÄ validator_section2.py      # Valida√ß√£o Se√ß√£o 2
‚îÇ   ‚îú‚îÄ‚îÄ logger.py                  # Sistema de logs (PostgreSQL/SQLite)
‚îÇ   ‚îú‚îÄ‚îÄ automate_release.py        # Automa√ß√£o screenshots/v√≠deo
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt           # Depend√™ncias de produ√ß√£o
‚îÇ   ‚îî‚îÄ‚îÄ requirements-dev.txt       # Depend√™ncias de desenvolvimento
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ index.html           # Interface principal do chat
‚îÇ   ‚îî‚îÄ‚îÄ logs.html            # Dashboard de logs
‚îú‚îÄ‚îÄ .env                     # Vari√°veis de ambiente (RAIZ)
‚îú‚îÄ‚îÄ CHANGELOG.md             # Hist√≥rico de vers√µes
‚îú‚îÄ‚îÄ README.md                # Documenta√ß√£o principal
‚îú‚îÄ‚îÄ CLAUDE.md                # Este arquivo
‚îî‚îÄ‚îÄ render.yaml              # Configura√ß√£o do Render
```

## Comandos para Desenvolvimento Local

```bash
# Terminal 1 - Backend (rodar do diret√≥rio RAIZ do projeto)
cd C:\AI\bo-assistant  # ou caminho do seu projeto
.\backend\venv\Scripts\activate      # Windows
source backend/venv/bin/activate     # Mac/Linux
python -m uvicorn backend.main:app --reload --host 0.0.0.0 --port 8000

# Terminal 2 - Frontend
cd docs
python -m http.server 3000 --bind 127.0.0.1

# Acessar: http://127.0.0.1:3000 ou http://localhost:3000
# (Se localhost n√£o funcionar, use 127.0.0.1 diretamente)
```

**IMPORTANTE:** O backend DEVE ser rodado do diret√≥rio raiz do projeto para que o arquivo `.env` seja carregado corretamente.

## Fluxo da Aplica√ß√£o

1. Usu√°rio inicia sess√£o ‚Üí `POST /new_session` ‚Üí retorna `session_id` e `bo_id`
2. **Se√ß√£o 1 (Contexto da Ocorr√™ncia)**: Sistema faz 6 perguntas sequenciais
3. Cada resposta √© validada pelo `validator.py`
4. Respostas v√°lidas s√£o armazenadas no `state_machine.py`
5. Ap√≥s 6 respostas, `llm_service.py` gera texto via Gemini ou Groq
6. **Se√ß√£o 2 (Abordagem a Ve√≠culo)**: Usu√°rio clica "Iniciar Se√ß√£o 2"
7. Sistema faz 8 perguntas (validadas por `validator_section2.py`)
8. Ap√≥s 8 respostas, novo texto √© gerado
9. Todos os eventos s√£o logados no banco via `logger.py`

## Perguntas da Se√ß√£o 1 (Contexto)

1. **1.1** - Dia, data e hora do acionamento
2. **1.2** - Composi√ß√£o da guarni√ß√£o e prefixo
3. **1.3** - Natureza do empenho
4. **1.4** - Ordem de servi√ßo / COPOM / DDU
5. **1.5** - Local exato da ocorr√™ncia
6. **1.6** - Hist√≥rico do local / fac√ß√£o

## Perguntas da Se√ß√£o 2 (Abordagem a Ve√≠culo)

1. **2.0** - Havia ve√≠culo? (se N√ÉO, pula se√ß√£o)
2. **2.1** - Marca e modelo do ve√≠culo
3. **2.2** - Placa do ve√≠culo (formato Mercosul)
4. **2.3** - Ocupantes (gradua√ß√£o, nome, fun√ß√£o)
5. **2.4** - Posi√ß√£o do ve√≠culo
6. **2.5** - Atitude do condutor
7. **2.6** - Descri√ß√£o da abordagem
8. **2.7** - Motivo da suspei√ß√£o

## Principais Endpoints da API

| M√©todo | Endpoint | Descri√ß√£o |
|--------|----------|-----------|
| POST | `/new_session` | Inicia nova sess√£o |
| POST | `/chat` | Processa resposta do usu√°rio |
| PUT | `/chat/{session_id}/answer/{step}` | Edita resposta anterior |
| POST | `/feedback` | Registra feedback (üëçüëé) |
| GET | `/api/logs` | Lista sess√µes |
| GET | `/api/stats` | Estat√≠sticas gerais |

## Vari√°veis de Ambiente

```bash
# .env (na RAIZ do projeto, n√£o em backend/)
GEMINI_API_KEY=sua_chave_aqui
GROQ_API_KEY=sua_chave_groq_aqui
DATABASE_URL=postgresql://...  # Apenas em produ√ß√£o
```

**IMPORTANTE:** O arquivo `.env` deve estar na raiz do projeto (`C:\AI\bo-assistant\.env`) para ser carregado corretamente pelo backend.

## Princ√≠pios de Desenvolvimento

1. **Nunca inventar informa√ß√µes** - O LLM s√≥ usa dados fornecidos pelo usu√°rio
2. **Valida√ß√£o inteligente** - Rejeita respostas vagas sem ser excessivamente r√≠gido
3. **Encoding UTF-8** - Sempre usar UTF-8 em arquivos Python (acentos!)
4. **C√≥digo simples** - JavaScript vanilla, sem frameworks complexos

## Vers√£o Atual

**v0.6.4** (20/12/2025)
- ‚úÖ Corre√ß√£o cr√≠tica: Sincroniza√ß√£o backend durante restaura√ß√£o de rascunhos
- ‚úÖ Sistema de rascunhos 100% funcional para Se√ß√£o 1 e Se√ß√£o 2
- Backend atualiza `currentQuestionStep` durante loop de sincroniza√ß√£o
- Valida√ß√£o de respostas alinhada com pergunta apresentada
- Suporte ao Groq API (Llama 3.3 70B) - 14.400 req/dia
- Arquitetura multi-provider (Gemini + Groq)

## Equipe

- **Cristiano Maia** - Delivery Manager & Tech Lead
- **Claudio Moreira** - Especialista em Reda√ß√£o de BOs (Sargento PM)

## Notas Importantes

- O backend no Render (free tier) "dorme" ap√≥s 15 min de inatividade
- Primeira requisi√ß√£o pode demorar 30-60s para "acordar"
- Frontend √© est√°tico no GitHub Pages (deploy autom√°tico no push)
- Testar localmente ANTES de fazer push para produ√ß√£o

---

## üêõ Debugging Tips

### Backend n√£o est√° gerando texto (Erro 500)

**Diagn√≥stico:**
1. Verificar se API keys est√£o carregadas:
   - Adicionar print tempor√°rio em `llm_service.py.__init__()`:
   ```python
   print(f"DEBUG: gemini_key = {os.getenv('GEMINI_API_KEY')[:10]}...")
   print(f"DEBUG: groq_key = {os.getenv('GROQ_API_KEY')[:10]}...")
   ```
   - Se retornar `None`, arquivo `.env` n√£o est√° sendo carregado

2. Verificar CWD (current working directory):
   - `python-dotenv` carrega `.env` do diret√≥rio onde o comando foi executado
   - **ERRADO:** `cd backend && uvicorn main:app` (procura `.env` em `backend/`)
   - **CORRETO:** `python -m uvicorn backend.main:app` (procura `.env` na raiz)

3. Verificar se porta 8000 est√° livre:
   ```bash
   # Windows
   netstat -ano | findstr :8000
   taskkill /F /IM python.exe

   # Mac/Linux
   lsof -i :8000
   kill -9 <PID>
   ```

### Frontend conectando ao Render em vez de localhost

**Problema:** DevTools mostra requisi√ß√µes indo para `bo-assistant-backend.onrender.com` mesmo rodando localmente.

**Causa:** C√≥digo JavaScript detecta apenas `localhost`, n√£o `127.0.0.1`.

**Solu√ß√£o (j√° implementada na v0.6.1):**
```javascript
const API_URL = (window.location.hostname === 'localhost' || window.location.hostname === '127.0.0.1')
    ? 'http://localhost:8000'
    : 'https://bo-assistant-backend.onrender.com';
```

### Endpoint de edi√ß√£o retornando erro 500

**Sintoma:** `ValueError: too many values to unpack (expected 2)`

**Causa:** Sess√µes foram refatoradas de tupla para dict, mas endpoint de edi√ß√£o n√£o foi atualizado.

**Como debugar:**
1. Verificar estrutura em `main.py`:
   ```python
   print(f"DEBUG: sessions[session_id] = {sessions[session_id]}")
   ```
2. Estrutura correta (v0.5.0+):
   ```python
   {
       "bo_id": "BO-20251220-xxxxx",
       "sections": {
           1: StateMachine(),
           2: StateMachineSection2()
       }
   }
   ```

### Automa√ß√£o de screenshots com problemas

**Problema 1:** Element n√£o √© clic√°vel
- **Solu√ß√£o:** Usar `wait_for_selector(..., state='visible')` antes de interagir

**Problema 2:** Screenshot mostra √°rea errada
- **Causa:** Scroll executado antes de a√ß√£o que tamb√©m causa scroll
- **Solu√ß√£o:** Executar a√ß√µes ‚Üí aguardar efeitos ‚Üí scroll ‚Üí screenshot

**Problema 3:** Sidebar/modal com conte√∫do sobreposto
- **Causa:** `full_page=True` faz scroll virtual, elementos fixed aparecem atrav√©s
- **Solu√ß√£o:** Usar `full_page=False` para overlays

### Quota do LLM excedida

**Sintoma:** Erro 429 ou "rate_limit" na mensagem.

**Solu√ß√µes:**
- Gemini 2.5 Flash: 20 req/dia (free tier)
- Groq Llama 3.3 70B: 14.400 req/dia (free tier) - **Recomendado para testes**
- Trocar provider no frontend (`index.html` linhas 520, 1149, 1408): `llm_provider: 'groq'`

### Logs de debug tempor√°rios

**Boas pr√°ticas:**
1. Sempre adicionar coment√°rio `# DEBUG - remover antes do commit`
2. Usar prefixo claro: `print(f"DEBUG GROQ ERROR: {error}")`
3. Limpar antes de fazer merge para main
4. Evitar deixar prints em produ√ß√£o (poluem logs do Render)

---
